---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 24 @ 11:59PM
author: Sylvia Wang, 105118268
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
---

Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(gtsummary))
```

## Predicting 30-day mortality

Using the ICU cohort `icu_cohort.rds` you built in Homework 3, develop at least three analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression with elastic net (lasso + ridge) penalty (e.g., glmnet or keras package), (2) random forest, (3) boosting, and (4) support vector machines, or (5) MLP neural network (keras package)

Data preparation:
Create a reduced dataset containing only demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, first vital measurements during ICU stay, and thirty day mortality indicator.
For lab and vital measurements, extreme outliers (those beyond the outer fence, i.e., +/-3*IQR) were reassigned as `NA`'s, because some obvious unrealistic values have been identified from the data (e.g. a negative heart rate or a heart rate of zero, etc.).

```{r}
icu_cohort <- readRDS("icu_cohort.rds") 
icu_cohort_reduced <- icu_cohort %>% 
  select("gender", "age_hadm", "marital_status", "ethnicity", "sodium",
         "chloride", "potassium", "bicarbonate", "hematocrit", "creatinine",
         "wbc_count", "glucose", "bp_systolic", "bp_mean", "resp_rate",
         "heart_rate", "temp", "thirty_day_mort") 

vars <- c("sodium", "chloride", "potassium", "bicarbonate", "hematocrit",
          "creatinine", "wbc_count", "glucose", "bp_systolic", "bp_mean", 
          "resp_rate", "heart_rate", "temp")
outlier_removed <- function(x){
    p25 <- quantile(x, probs = 0.25, na.rm=T)
    p75 <- quantile(x, probs = 0.75, na.rm=T)
    IQR <- p75 - p25
    x <- ifelse(x > p75 + 3 * IQR | x < p25 - 3 * IQR , NA, x)
}
lab_vital <- icu_cohort_reduced %>% 
  select(all_of(vars)) %>% 
  apply(2, outlier_removed)
icu_cohort_reduced[, colnames(lab_vital)] <- lab_vital
icu_cohort_reduced %>% tbl_summary(by = thirty_day_mort)
```

### Partition data into 50% training set and 50% test set. Stratify partitioning according the 30-day mortality status.

```{r}
icu_cohort_reduced$thirty_day_mort <- 
  as.factor(icu_cohort_reduced$thirty_day_mort)

set.seed(203)
data_split <- initial_split(
  icu_cohort_reduced, 
  strata = "thirty_day_mort", 
  prop = 0.5)
data_split

train <- training(data_split)
dim(train)
test <- testing(data_split)
dim(test)
```

### Train and tune the models using the training set.

#### Method 1: Logistic regression with elastic net regularization

```{r}
# Recipe and Preprocessing:
sapply(icu_cohort_reduced, class)
logit_recipe <- recipe(thirty_day_mort ~ ., data = train) %>%
  step_impute_mean(sodium) %>%
  step_impute_mean(chloride) %>%
  step_impute_mean(potassium) %>%
  step_impute_mean(bicarbonate) %>%
  step_impute_mean(hematocrit) %>%
  step_impute_mean(creatinine) %>%
  step_impute_mean(wbc_count) %>%
  step_impute_mean(glucose) %>%
  step_impute_mean(bp_systolic) %>%
  step_impute_mean(bp_mean) %>%
  step_impute_mean(resp_rate) %>%
  step_impute_mean(heart_rate) %>%
  step_impute_mean(temp) %>%
  step_impute_mode(marital_status) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>%
  prep(training = train, retain = TRUE)
logit_recipe

# Create model:
logit_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet", standardize = FALSE)
logit_mod

# Bundle the recipe and model:
logit_wf <- workflow() %>%
  add_recipe(logit_recipe) %>%
  add_model(logit_mod)
logit_wf

# Tuning grid:
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5))

# Cross validation:
set.seed(203)
folds <- vfold_cv(train, v = 5)
folds

# Fit cross validation:
system.time({
logit_fit <- logit_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
})
logit_fit

# Visualize CV results:
logit_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = penalty, y = mean, color = mixture)) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()

# Show the top 5 models:
logit_fit %>% show_best("roc_auc")

# Select the best model:
best_logit <- logit_fit %>% select_best("roc_auc")
best_logit

# Finalize model:
# Final workflow:
final_wf <- logit_wf %>% finalize_workflow(best_logit)
final_wf

# Fit the whole training set and predict the test cases:
final_fit <- final_wf %>% last_fit(data_split)
final_fit

# Test metrics:
final_fit %>% collect_metrics()
```

#### Method 2: Random forest

```{r}
# Recipe and preprocessing:
rf_recipe <- recipe(thirty_day_mort ~ ., data = train) %>%
  step_impute_mean(sodium) %>%
  step_impute_mean(chloride) %>%
  step_impute_mean(potassium) %>%
  step_impute_mean(bicarbonate) %>%
  step_impute_mean(hematocrit) %>%
  step_impute_mean(creatinine) %>%
  step_impute_mean(wbc_count) %>%
  step_impute_mean(glucose) %>%
  step_impute_mean(bp_systolic) %>%
  step_impute_mean(bp_mean) %>%
  step_impute_mean(resp_rate) %>%
  step_impute_mean(heart_rate) %>%
  step_impute_mean(temp) %>%
  step_impute_mode(marital_status) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = train, retain = TRUE)
rf_recipe

# Create model:
rf_mod <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune()) %>% 
  set_engine("ranger")
rf_mod

# Create workflow:
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf

# Tuning grid:
param_grid_rf <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid_rf

# Cross validation:
folds # created in method 1

# Fit cross validation:
system.time({
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid_rf,
    metrics = metric_set(roc_auc, accuracy)
    )
})
rf_fit

# Visualize CV results:
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_point() + 
  labs(x = "Num. of Trees", y = "CV AUC")

# Show the top 5 models:
rf_fit %>% show_best("roc_auc")

# Select the best model:
best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf

# Finalize the model:
# Final workflow:
final_wf_rf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf_rf

# Fit the whole training set and predict the test cases
final_fit_rf <- final_wf_rf %>%
  last_fit(data_split)
final_fit_rf

# Test metrics
final_fit_rf %>% collect_metrics()
```

#### Method 3: Extreme gradient boosting (XGBoost)

```{r}
# Recipe and preprocessing:
gb_recipe <- recipe(thirty_day_mort ~ ., data = train) %>%
  step_impute_mean(sodium) %>%
  step_impute_mean(chloride) %>%
  step_impute_mean(potassium) %>%
  step_impute_mean(bicarbonate) %>%
  step_impute_mean(hematocrit) %>%
  step_impute_mean(creatinine) %>%
  step_impute_mean(wbc_count) %>%
  step_impute_mean(glucose) %>%
  step_impute_mean(bp_systolic) %>%
  step_impute_mean(bp_mean) %>%
  step_impute_mean(resp_rate) %>%
  step_impute_mean(heart_rate) %>%
  step_impute_mean(temp) %>%
  step_impute_mode(marital_status) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = train, retain = TRUE)
gb_recipe

# Create model:
gb_mod <- boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()) %>% 
  set_engine("xgboost")
gb_mod

# Create workflow:
gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf

# Tuning grid:
param_grid_gb <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10))
param_grid_gb

# Cross validation:
folds # created in method 1

# Fit cross validation:
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid_gb,
    metrics = metric_set(roc_auc, accuracy))
gb_fit

# Visualize CV results:
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()

# Show the top 5 models:
gb_fit %>% show_best("roc_auc")

# Select the best model:
best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb

# Finalize the model:
# Final workflow:
final_wf_gb <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf_gb

# Fit the whole training set and predict the test cases
final_fit_gb <- final_wf_gb %>%
  last_fit(data_split)
final_fit_gb

# Test metrics
final_fit_gb %>% collect_metrics()
```

#### Method 4: Support vector machines (SVM) with radial basis function (RBF) Kernel (I did not finish the step fitting the workflow to the CV due to extremely long running time)

```{r}
# Recipe and preprocessing:
svm_recipe <- recipe(thirty_day_mort ~ ., data = train) %>%
  step_impute_mean(sodium) %>%
  step_impute_mean(chloride) %>%
  step_impute_mean(potassium) %>%
  step_impute_mean(bicarbonate) %>%
  step_impute_mean(hematocrit) %>%
  step_impute_mean(creatinine) %>%
  step_impute_mean(wbc_count) %>%
  step_impute_mean(glucose) %>%
  step_impute_mean(bp_systolic) %>%
  step_impute_mean(bp_mean) %>%
  step_impute_mean(resp_rate) %>%
  step_impute_mean(heart_rate) %>%
  step_impute_mean(temp) %>%
  step_impute_mode(marital_status) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = train, retain = TRUE)
svm_recipe

# Create model:
svm_mod <- svm_rbf(
    mode = "classification",
    cost = tune(),
    rbf_sigma = tune()) %>% 
  set_engine("kernlab")
svm_mod

# Create workflow:
svm_wf <- workflow() %>%
  add_recipe(svm_recipe) %>%
  add_model(svm_mod)
svm_wf

# Tuning grid:
param_grid_svm <- grid_regular(
  cost(range = c(-8, 5)),
  rbf_sigma(range = c(-5, -3)),
  levels = c(14, 5))
param_grid_svm

# Cross validation:
folds # created in method 1

# I was not able to finish running the code below as it did not finish in hours.

# # Fit cross validation:
# system.time({
# svm_fit <- svm_wf %>%
#   tune_grid(
#     resamples = folds,
#     grid = param_grid_svm,
#     metrics = metric_set(roc_auc, accuracy))
# })
# svm_fit
# 
# # Visualize CV results:
# svm_fit %>%
#   collect_metrics() %>%
#   print(width = Inf) %>%
#   filter(.metric == "roc_auc") %>%
#   ggplot(mapping = aes(x = cost, y = mean, alpha = rbf_sigma)) +
#   geom_point() +
#   labs(x = "Cost", y = "CV AUC") +
#   scale_x_log10()
# 
# # Show the top 5 models:
# svm_fit %>% show_best("roc_auc")
# 
# # Select the best model:
# best_svm <- svm_fit %>%
#   select_best("roc_auc")
# best_svm
# 
# # Finalize the model:
# # Final workflow:
# final_wf_svm <- svm_wf %>%
#   finalize_workflow(best_svm)
# final_wf_svm
# 
# # Fit the whole training set and predict the test cases
# final_fit_svm <- final_wf_svm %>%
#   last_fit(data_split)
# final_fit_svm
# 
# # Test metrics
# final_fit_svm %>% collect_metrics()
```

#### Method 5: Multilayer perceptron (MLP)

```{r}
# Recipe and preprocessing:
mlp_recipe <- recipe(thirty_day_mort ~ ., data = train) %>%
  step_impute_mean(sodium) %>%
  step_impute_mean(chloride) %>%
  step_impute_mean(potassium) %>%
  step_impute_mean(bicarbonate) %>%
  step_impute_mean(hematocrit) %>%
  step_impute_mean(creatinine) %>%
  step_impute_mean(wbc_count) %>%
  step_impute_mean(glucose) %>%
  step_impute_mean(bp_systolic) %>%
  step_impute_mean(bp_mean) %>%
  step_impute_mean(resp_rate) %>%
  step_impute_mean(heart_rate) %>%
  step_impute_mean(temp) %>%
  step_impute_mode(marital_status) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  prep(training = train, retain = TRUE)
mlp_recipe

# Create model:
mlp_mod <- mlp(
  mode = "classification",
  hidden_units = tune(),
  dropout = tune(),
  epochs = 50) %>% 
  set_engine("keras", verbose = 0)
mlp_mod

# Create workflow:
mlp_wf <- workflow() %>%
  add_recipe(mlp_recipe) %>%
  add_model(mlp_mod)
mlp_wf

# Tuning grid:
param_grid_mlp <- grid_regular(
  hidden_units(range = c(1, 20)),
  dropout(range = c(0, 0.6)),
  levels = 5)
param_grid_mlp

# Cross validation:
folds # created in method 1

# Fit cross validation:
system.time({
mlp_fit <- mlp_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid_mlp,
    metrics = metric_set(roc_auc, accuracy))
})
mlp_fit

# Visualize CV results:
mlp_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = dropout, y = mean, alpha = hidden_units)) +
  geom_point() +
  labs(x = "Dropout Rate", y = "CV AUC") +
  scale_x_log10()

# Show the top 5 models:
mlp_fit %>% show_best("roc_auc")

# Select the best model:
best_mlp <- mlp_fit %>%
  select_best("roc_auc")
best_mlp

# Finalize the model:
# Final workflow:
final_wf_mlp <- mlp_wf %>%
  finalize_workflow(best_mlp)
final_wf_mlp

# Fit the whole training set and predict the test cases
final_fit_mlp <- final_wf_mlp %>%
  last_fit(data_split)
final_fit_mlp

# Test metrics
final_fit_mlp %>% collect_metrics()
```

### Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each model.

```{r}
# Logistic regression with elastic net regularization:
final_fit %>% collect_metrics()
# Random forest:
final_fit_rf %>% collect_metrics()
# XGBoost:
final_fit_gb %>% collect_metrics()
# MLP:
final_fit_mlp %>% collect_metrics()
```

The accuracy and area under the ROC curve of the logistic regression with elastic net regulation model are 0.9076576 and 0.8068327, respectively.

The accuracy and area under the ROC curve of the random forest model are 0.9239511 and 0.8808497, respectively.

The accuracy and area under the ROC curve of the XGBoost model are 0.9315522 and 0.8909976, respectively.

I was not able to finish running the SVM with RBF Kernel model as it was taking hours.

The accuracy and area under the ROC curve of the MLP model are 0.8963688 and 0.7702331, respectively.

Overall, among the models that were run, the XGBoost model showed the best model classification performance on the test set, as indicated by both its accuracy and its area under the ROC curve.